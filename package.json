{
  "name": "llm-interrogator",
  "version": "1.0.0",
  "description": "LLM Interrogator - Extract concepts from LLM training data",
  "workspaces": ["frontend"],
  "scripts": {
    "setup": "python3 -m venv venv && ./venv/bin/pip install -r requirements.txt && npm install && test -f .env || cp .env.example .env",
    "kill": "pkill -f 'python.*app.py' 2>/dev/null || true",
    "start": "npm run kill && npm run build && concurrently \"npm run backend\" \"npm run open\"",
    "dev": "npm run kill && concurrently \"npm run backend\" \"npm run frontend:dev\" \"sleep 2 && npm run open\"",
    "backend": "./venv/bin/python app.py",
    "frontend:dev": "npm run dev -w frontend",
    "build": "npm run build -w frontend",
    "open": "sleep 1 && (open http://localhost:5001 || xdg-open http://localhost:5001 || start http://localhost:5001)",
    "test": "./venv/bin/python -m pytest tests/ -v",
    "test:fast": "./venv/bin/python -m pytest tests/ -v -m 'not integration'",
    "test:all": "./venv/bin/python -m pytest tests/ -v --tb=short"
  },
  "devDependencies": {
    "concurrently": "^8.2.0"
  }
}
